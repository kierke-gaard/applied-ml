{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How wrote this?\n",
    "\n",
    "\"I ought to be thy Adam, but I am rather the fallen angel.\"\n",
    "\n",
    "\"Nothing is so painful to the human mind as a great and sudden change.\"\n",
    "\n",
    "\"It is the nature of truth in general, as of some ores in particular, to be richest when most superficial.\"\n",
    "\n",
    "\"The oldest and strongest emotion of mankind is fear, and the oldest and strongest kind of fear is fear of the unknown.\"\n",
    "\n",
    "![alt text](img/shelley.jpg) ![alt text](img/poe.png) ![alt text](img/lovecraft.jpg)\n",
    "\n",
    "Edgar Allan Poe, Mary Shelley, Howard Phillips Lovecraft\n",
    "\n",
    "# A Study for Authorship Attribution on Sentences\n",
    "\n",
    "Objective:\n",
    "Identify the writer of a sentence\n",
    "\n",
    "Data:\n",
    "For each writer a list of sentences from works of e.g. like Frankenstein, The Unparalleled Adventure of Hans Pfaall, The Call of Cthulhu\n",
    "\n",
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "from os import path\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from functools import reduce, partial\n",
    "from itertools import groupby\n",
    "import operator\n",
    "\n",
    "# Utilities\n",
    "identity = lambda x: x\n",
    "\n",
    "def compose(*functions):\n",
    "    return reduce(lambda f, g: lambda x: f(g(x)),\n",
    "                            functions,\n",
    "                            identity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>[This, process, ,, however, ,, afforded, me, n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>[It, never, once, occurred, to, me, that, the,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>[In, his, left, hand, was, a, gold, snuff, box...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "      <td>[How, lovely, is, spring, As, we, looked, from...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>[Finding, nothing, else, ,, not, even, gold, ,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author  \\\n",
       "0  id26305  This process, however, afforded me no means of...    EAP   \n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL   \n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP   \n",
       "3  id27763  How lovely is spring As we looked from Windsor...    MWS   \n",
       "4  id12958  Finding nothing else, not even gold, the Super...    HPL   \n",
       "\n",
       "                                              tokens  \n",
       "0  [This, process, ,, however, ,, afforded, me, n...  \n",
       "1  [It, never, once, occurred, to, me, that, the,...  \n",
       "2  [In, his, left, hand, was, a, gold, snuff, box...  \n",
       "3  [How, lovely, is, spring, As, we, looked, from...  \n",
       "4  [Finding, nothing, else, ,, not, even, gold, ,...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Data and Tokenize\n",
    "d = pd.read_csv('data/train.csv')\n",
    "#execute only once to download resources\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('wordnet')\n",
    "d['tokens'] = d.text.apply(nltk.word_tokenize)\n",
    "d.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Further provisions of token preprocessing\n",
    "\n",
    "# Remove heading apostrophes as there occurrence is highly unbalanced among wirter corpora (might be due to publishing reasons)\n",
    "filter_heading_apostrophes = compose(list,\n",
    "                                     partial(filter, lambda tokens: \"'\"!=tokens[0][0]))\n",
    "\n",
    "transform_lower_case = compose(list,\n",
    "                               partial(map, lambda token: token.lower()))\n",
    "\n",
    "d.tokens = d.tokens.apply(compose(filter_heading_apostrophes,\n",
    "                                  transform_lower_case))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary in the Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenList = partial(reduce, operator.concat)\n",
    "frequencies = lambda l: sorted([(k, len(list(v))) for k,v in groupby(sorted(l))],\n",
    "                               key=lambda x: x[1], reverse=True)\n",
    "\n",
    "allTokens = {}\n",
    "allTokens['corpus'] = tokenList(d.tokens)\n",
    "vocabulary = {}\n",
    "vocabulary['corpus'] = set(allTokens['corpus'])\n",
    "tokenFreq = {}\n",
    "tokenFreq['corpus'] = frequencies(allTokens['corpus'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Whole Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens 589810 , vocabulary size 25127\n",
      "Most occurring tokens [(',', 38220), ('the', 35559), ('of', 20953), ('.', 19119), ('and', 17953), ('to', 12839), ('i', 10794), ('a', 10720), ('in', 9457), ('was', 6662)]\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of tokens\", len(allTokens['corpus']), \n",
    "      \", vocabulary size\", len(vocabulary['corpus']))\n",
    "print(\"Most occurring tokens\", tokenFreq['corpus'][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vocabulary of Writers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "For writer EAP\n",
      "  Number of tokens 229700\n",
      "  Vocabulary size 15306\n",
      "  Most frequent words: [(',', 17594), ('the', 14969), ('of', 8970), ('.', 7700), ('and', 5733), ('to', 4761), ('a', 4715), ('in', 4124), ('i', 3778), ('that', 2327), ('it', 2326), ('was', 2229), ('my', 1788), ('with', 1695), ('is', 1668), ('``', 1628), ('at', 1588), ('as', 1570), ('which', 1488), (';', 1354), ('not', 1347), ('for', 1343), ('had', 1318), ('he', 1302), ('this', 1296), ('his', 1278), ('by', 1206), ('but', 1200), ('be', 1097), ('have', 1055)]\n",
      "\n",
      "For writer HPL\n",
      "  Number of tokens 172378\n",
      "  Vocabulary size 14522\n",
      "  Most frequent words: [('the', 10933), (',', 8581), ('and', 6098), ('of', 5846), ('.', 5707), ('a', 3294), ('to', 3249), ('in', 2736), ('i', 2704), ('was', 2184), ('that', 2021), ('had', 1783), ('he', 1647), ('it', 1402), ('as', 1173), ('his', 1171), (';', 1143), ('with', 1122), ('for', 1020), ('but', 979), ('my', 971), ('at', 940), ('on', 933), ('which', 920), ('from', 910), ('not', 894), ('were', 708), ('by', 661), ('they', 648), ('an', 645)]\n",
      "\n",
      "For writer MWS\n",
      "  Number of tokens 187732\n",
      "  Vocabulary size 11541\n",
      "  Most frequent words: [(',', 12045), ('the', 9657), ('of', 6137), ('and', 6122), ('.', 5712), ('to', 4829), ('i', 4312), ('a', 2711), (';', 2662), ('my', 2659), ('in', 2597), ('was', 2249), ('that', 2091), ('her', 1657), ('his', 1646), ('with', 1529), ('he', 1484), ('me', 1473), ('had', 1330), ('not', 1189), ('it', 1180), ('but', 1172), ('for', 1131), ('as', 1097), ('on', 1044), ('you', 1044), ('by', 995), ('from', 968), ('which', 961), ('she', 924)]\n"
     ]
    }
   ],
   "source": [
    "writers = ['EAP', 'HPL', 'MWS']\n",
    "for writer in writers:\n",
    "    print('\\nFor writer', writer)\n",
    "    allTokens[writer] = tokenList(d[d.author==writer].tokens)\n",
    "    print('  Number of tokens', len(allTokens[writer]))\n",
    "    vocabulary[writer] = set(allTokens[writer])\n",
    "    print('  Vocabulary size', len(vocabulary[writer]))\n",
    "    tokenFreq[writer] = frequencies(allTokens[writer])\n",
    "    print('  Most frequent words:', tokenFreq[writer][:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Tokens occuring only once 9307\n",
      "Samples of rare words ['a.d', 'a.d.', 'a.m', 'aaem', 'ab', 'abaft', 'abased', 'abasement', 'abashed', 'abashment']\n"
     ]
    }
   ],
   "source": [
    "singletons = list(filter(lambda x: x[1] == 1, tokenFreq['corpus']))\n",
    "print(\"Number of Tokens occuring only once\", len(singletons))\n",
    "print(\"Samples of rare words\", list(map(lambda x: x[0], singletons[:10])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple model on most frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "dv = DictVectorizer()\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 100 most frequent tokens are [',', 'the', 'of', '.', 'and', 'to', 'i', 'a', 'in', 'was', 'that', 'my', ';', 'it', 'he', 'had', 'with', 'his', 'as', 'for', 'not', 'which', 'but', 'at', 'me', 'from', 'by', '``', 'is', 'this', 'on', 'be', 'her', 'were', 'have', 'all', 'you', 'an', 'we', 'or', 'no', 'one', 'so', 'him', 'when', 'they', 'been', 'upon', 'there', 'could', 'she', 'its', 'would', 'more', 'now', 'their', '?', 'what', 'some', 'our', 'are', 'into', 'than', 'will', 'very', 'who', 'if', 'them', 'only', 'then', 'up', 'these', 'before', 'man', 'about', 'any', 'time', 'did', 'yet', 'out', 'said', 'even', 'your', 'might', 'after', 'do', 'old', 'like', 'can', 'first', 'must', 'us', 'most', 'through', 'over', 'never', 'life', 'night', 'made', 'other']\n",
      "  with a coverage of 335389\n"
     ]
    }
   ],
   "source": [
    "# Vectorize\n",
    "def first_n_most_frequent_tokens(n):\n",
    "    return list(map(lambda x: x[0], tokenFreq['corpus'][:n]))\n",
    "\n",
    "n = 100\n",
    "print(\"The\", n, \"most frequent tokens are\", first_n_most_frequent_tokens(n))\n",
    "print(\"  with a coverage of\", reduce(lambda x, y: x + y[1],\n",
    "                                         tokenFreq['corpus'][:n], 0))\n",
    "\n",
    "filter_by_set = lambda coll: compose(list,\n",
    "                                     partial(filter, lambda x: x in coll))\n",
    "\n",
    "def frequencies_in_tokens(tokens, vocabulary):\n",
    "    \"Returns the frequencies of the vocs in the tokens as sparse matrix\"\n",
    "    return dv.fit_transform(map(compose(dict,\n",
    "                                     frequencies,\n",
    "                                     filter_by_set(vocabulary)),\n",
    "                            tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = lambda x, y: sum(x == y) / len(x)\n",
    "\n",
    "def classifier_assessment(model_class, X, Y) -> None:\n",
    "    return {'in-sample-precision': precision(Y, model_class.fit(X,Y).predict(X)),\n",
    "            'out-of-sample-precision': np.mean(cross_val_score(model_class, X, Y, cv=8))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        EAP       0.67      0.68      0.67      7900\n",
      "        HPL       0.59      0.66      0.62      5635\n",
      "        MWS       0.67      0.59      0.63      6044\n",
      "\n",
      "avg / total       0.65      0.65      0.65     19579\n",
      "\n",
      "Confusion Matrix\n",
      " ['EAP', 'HPL', 'MWS'] \n",
      " [[5350 1414 1136]\n",
      " [1307 3701  627]\n",
      " [1325 1135 3584]]\n"
     ]
    }
   ],
   "source": [
    "Y = d.author\n",
    "X = frequencies_in_tokens(d.tokens,\n",
    "                          first_n_most_frequent_tokens(200))\n",
    "model = MultinomialNB().fit(X,Y)\n",
    "Y_pred = model.predict(X)\n",
    "print(classification_report(Y, Y_pred))\n",
    "print('Confusion Matrix\\n', writers, '\\n', confusion_matrix(Y, Y_pred, labels=writers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sensitivity to Vocabulary Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 {'in-sample-precision': 0.45559017314469585, 'out-of-sample-precision': 0.45605126059258444}\n",
      "10 {'in-sample-precision': 0.4691250829970887, 'out-of-sample-precision': 0.4692259731151678}\n",
      "50 {'in-sample-precision': 0.5732672761632361, 'out-of-sample-precision': 0.5711207508283209}\n",
      "100 {'in-sample-precision': 0.5990091424485418, 'out-of-sample-precision': 0.5963534444116483}\n",
      "200 {'in-sample-precision': 0.6453342867357883, 'out-of-sample-precision': 0.6388973395539028}\n",
      "400 {'in-sample-precision': 0.6934981357576996, 'out-of-sample-precision': 0.6860909748901042}\n",
      "600 {'in-sample-precision': 0.7227641861177793, 'out-of-sample-precision': 0.7119357667044304}\n",
      "1000 {'in-sample-precision': 0.7562183972623729, 'out-of-sample-precision': 0.7413032593343052}\n",
      "1500 {'in-sample-precision': 0.7820113386791971, 'out-of-sample-precision': 0.7619378174824729}\n",
      "2000 {'in-sample-precision': 0.8008069870779917, 'out-of-sample-precision': 0.7786915094976463}\n",
      "5000 {'in-sample-precision': 0.8575514581950049, 'out-of-sample-precision': 0.822717144727947}\n",
      "10000 {'in-sample-precision': 0.8873282598702692, 'out-of-sample-precision': 0.8414120732064454}\n"
     ]
    }
   ],
   "source": [
    "for n in [5, 10, 50, 100, 200, 400, 600, 1000, 1500, 2000, 5000, 10000]:\n",
    "    X = frequencies_in_tokens(d.tokens, first_n_most_frequent_tokens(n))\n",
    "    print(n, classifier_assessment(MultinomialNB(), X, Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take no more than 200 words as from then on content bearings words appear, e.g. character names like Raymond.\n",
    "\n",
    "### Sensitivity towards Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True) {'in-sample-precision': 0.6453342867357883, 'out-of-sample-precision': 0.6388973395539028}\n",
      "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "     verbose=0) {'in-sample-precision': 0.6688288472342816, 'out-of-sample-precision': 0.6583091404139636}\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False) {'in-sample-precision': 0.9852903621226825, 'out-of-sample-precision': 0.5929291106550424}\n",
      "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "              learning_rate=0.1, loss='deviance', max_depth=5,\n",
      "              max_features=None, max_leaf_nodes=None,\n",
      "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "              min_samples_leaf=1, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False) {'in-sample-precision': 0.7260840696664794, 'out-of-sample-precision': 0.6625470032314047}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "n = 200\n",
    "X = frequencies_in_tokens(d.tokens,\n",
    "                          first_n_most_frequent_tokens(n))\n",
    "\n",
    "model_classes = [MultinomialNB(),\n",
    "                 LinearSVC(),\n",
    "                 RandomForestClassifier(),\n",
    "                 GradientBoostingClassifier(n_estimators= 100, max_depth = 5)]\n",
    "\n",
    "for model_class in model_classes:\n",
    "    print(model_class, '\\n', classifier_assessment(model_class, X, Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NearestCentroid(metric='euclidean', shrink_threshold=None) \n",
      " {'in-sample-precision': 0.477858930486746, 'out-of-sample-precision': 0.47617730584202467}\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=10, p=2,\n",
      "           weights='uniform') \n",
      " {'in-sample-precision': 0.6090198682261607, 'out-of-sample-precision': 0.5193303149288042}\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=20, p=2,\n",
      "           weights='uniform') \n",
      " {'in-sample-precision': 0.5833290770723735, 'out-of-sample-precision': 0.5329662552610975}\n"
     ]
    }
   ],
   "source": [
    "# Which of the common words have the most predictive power\n",
    "from sklearn.neighbors import NearestCentroid, KNeighborsClassifier\n",
    "for model_class in [NearestCentroid(),\n",
    "                    KNeighborsClassifier(n_neighbors=10),\n",
    "                    KNeighborsClassifier(n_neighbors=20)]:\n",
    "    print(model_class, '\\n', classifier_assessment(model_class, X, Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features with most predictive power\n",
    "In an multinomial model\n",
    "\\[ P_\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 200)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1model.feature_log_prob_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Enhance Features with Statistics\n",
    "Add to the above frequencies of common words the following statistics\n",
    " * sentence length\n",
    " \n",
    "The stastistics alone has predictive power. Find a ways for incorporation into above model approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'in-sample-precision': 0.40349353899586293,\n",
       " 'out-of-sample-precision': 0.4034935578860775}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_lengths = np.array(list(map(len, d.tokens))).reshape(-1, 1)\n",
    "classifier_assessment(MultinomialNB(), sentence_lengths, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#X = frequencies_in_tokens(d.tokens, first_n_most_frequent_tokens(n))\n",
    "type(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Questions\n",
    " * Why does the usual tf-idf approach not work for author attribution?\n",
    " * Explain why the in-sample and ou-of-sample error for random forest are so significant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlook\n",
    "\n",
    "Character Level\n",
    " * n-grams \n",
    "   * eg. 3-grams |No_| |o_o| |_on| |one|\n",
    " * punctuation\n",
    "   * eg. frequency of commas, semicolons, periods, quotation-marks\n",
    "\n",
    "Lexical Level\n",
    " * word occurences\n",
    "   * frequences\n",
    "   * indicators\n",
    " * word n-grams\n",
    "\n",
    "Syntactical Level\n",
    " * part of speech\n",
    "   word classes as lexical items: noun, verb, adjective, adverb, pronoun, preposition, conjunction, interjection, numeral, article, determiner\n",
    " * part of sentence\n",
    "   consituents as lexical items: subject, predicate, direct/indirect object, modifier, abvervial,\n",
    " * sentence \n",
    "\n",
    "Semantic\n",
    " *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the above multinomial model the use of common words in a sentence happens in the following way\n",
    "# A writer y picks a common words x_j with probability p_j\n",
    "#  \n",
    "# https://github.com/scikit-learn/scikit-learn/blob/55bf5d9/sklearn/naive_bayes.py#L630"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
