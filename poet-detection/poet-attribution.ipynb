{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Who wrote this? A Study for Authorship Attribution\n",
    "\n",
    "\"I ought to be thy Adam, but I am rather the fallen angel.\"\n",
    "\n",
    "\"Nothing is so painful to the human mind as a great and sudden change.\"\n",
    "\n",
    "\"It is the nature of truth in general, as of some ores in particular, to be richest when most superficial.\"\n",
    "\n",
    "\"The oldest and strongest emotion of mankind is fear, and the oldest and strongest kind of fear is fear of the unknown.\"\n",
    "\n",
    "![alt text](img/shelley.jpg) ![alt text](img/poe.png) ![alt text](img/lovecraft.jpg)\n",
    "\n",
    "Edgar Allan Poe, Mary Shelley, Howard Phillips Lovecraft\n",
    "\n",
    "**Objective:**\n",
    "Identify the writer of a sentence\n",
    "\n",
    "**Data:**\n",
    "For each writer a list of sentences from works of e.g. like Frankenstein, The Unparalleled Adventure of Hans Pfaall, The Call of Cthulhu\n",
    "\n",
    "# Data Exploration and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "from os import path\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import seaborn as sb\n",
    "from functools import reduce, partial\n",
    "from itertools import groupby\n",
    "import operator\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Utilities\n",
    "identity = lambda x: x\n",
    "first = lambda x: x[0]\n",
    "second = lambda x: x[1]\n",
    "\n",
    "def compose(*functions):\n",
    "    \"\"\"Returns a function as composition of fuctions: [f, g,.. ,h] -> f°g°..°h\"\"\"\n",
    "    return reduce(lambda f, g: lambda x: f(g(x)),\n",
    "                            functions,\n",
    "                            identity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization and Provisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>[This, process, ,, however, ,, afforded, me, n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>[It, never, once, occurred, to, me, that, the,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>[In, his, left, hand, was, a, gold, snuff, box...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "      <td>[How, lovely, is, spring, As, we, looked, from...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>[Finding, nothing, else, ,, not, even, gold, ,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author  \\\n",
       "0  id26305  This process, however, afforded me no means of...    EAP   \n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL   \n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP   \n",
       "3  id27763  How lovely is spring As we looked from Windsor...    MWS   \n",
       "4  id12958  Finding nothing else, not even gold, the Super...    HPL   \n",
       "\n",
       "                                              tokens  \n",
       "0  [This, process, ,, however, ,, afforded, me, n...  \n",
       "1  [It, never, once, occurred, to, me, that, the,...  \n",
       "2  [In, his, left, hand, was, a, gold, snuff, box...  \n",
       "3  [How, lovely, is, spring, As, we, looked, from...  \n",
       "4  [Finding, nothing, else, ,, not, even, gold, ,...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = pd.read_csv('data/train.csv')\n",
    "#execute only once to download resources\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('wordnet')\n",
    "d['tokens'] = d.text.apply(nltk.word_tokenize)\n",
    "d.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Further provisions of token preprocessing\n",
    "# Remove heading apostrophes as their occurrence is highly unbalanced among writer's corpora (might be due to publishing reasons)\n",
    "filter_heading_apostrophes = compose(list,\n",
    "                                     partial(filter, lambda tokens: \"'\"!=tokens[0][0]))\n",
    "\n",
    "transform_lower_case = compose(list,\n",
    "                               partial(map, lambda token: token.lower()))\n",
    "\n",
    "d.tokens = d.tokens.apply(compose(filter_heading_apostrophes,\n",
    "                                  transform_lower_case))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary in the Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenList = partial(reduce, operator.concat)\n",
    "frequencies = lambda l: sorted([(k, len(list(v))) for k,v in groupby(sorted(l))],\n",
    "                               key=second, reverse=True)\n",
    "\n",
    "allTokens = {}\n",
    "allTokens['corpus'] = tokenList(d.tokens)\n",
    "vocabulary = {}\n",
    "vocabulary['corpus'] = set(allTokens['corpus'])\n",
    "tokenFreq = {}\n",
    "tokenFreq['corpus'] = frequencies(allTokens['corpus'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Whole Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens 589810 , vocabulary size 25127\n",
      "Most occurring tokens [(',', 38220), ('the', 35559), ('of', 20953), ('.', 19119), ('and', 17953), ('to', 12839), ('i', 10794), ('a', 10720), ('in', 9457), ('was', 6662)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "25126"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Number of tokens\", len(allTokens['corpus']), \n",
    "      \", vocabulary size\", len(vocabulary['corpus']))\n",
    "print(\"Most occurring tokens\", tokenFreq['corpus'][:10])\n",
    "#plot frequency distribution\n",
    "#sb.distplot(allTokens['corpus']);\n",
    "#plt.plot(x = list(range(len(tokenFreq.values()))), y = tokenFreq.values())\n",
    "#list(zip(range(len(tokenFreq['corpus'])),\n",
    "#         (map, first, tokenFreq['corpus'])))[:10]\n",
    "list(range(len(tokenFreq['corpus'])))[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vocabulary of Writers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "For writer EAP\n",
      "  Number of tokens 229700\n",
      "  Vocabulary size 15306\n",
      "  Most frequent words: [(',', 17594), ('the', 14969), ('of', 8970), ('.', 7700), ('and', 5733), ('to', 4761), ('a', 4715), ('in', 4124), ('i', 3778), ('that', 2327), ('it', 2326), ('was', 2229), ('my', 1788), ('with', 1695), ('is', 1668), ('``', 1628), ('at', 1588), ('as', 1570), ('which', 1488), (';', 1354), ('not', 1347), ('for', 1343), ('had', 1318), ('he', 1302), ('this', 1296), ('his', 1278), ('by', 1206), ('but', 1200), ('be', 1097), ('have', 1055)]\n",
      "\n",
      "For writer HPL\n",
      "  Number of tokens 172378\n",
      "  Vocabulary size 14522\n",
      "  Most frequent words: [('the', 10933), (',', 8581), ('and', 6098), ('of', 5846), ('.', 5707), ('a', 3294), ('to', 3249), ('in', 2736), ('i', 2704), ('was', 2184), ('that', 2021), ('had', 1783), ('he', 1647), ('it', 1402), ('as', 1173), ('his', 1171), (';', 1143), ('with', 1122), ('for', 1020), ('but', 979), ('my', 971), ('at', 940), ('on', 933), ('which', 920), ('from', 910), ('not', 894), ('were', 708), ('by', 661), ('they', 648), ('an', 645)]\n",
      "\n",
      "For writer MWS\n",
      "  Number of tokens 187732\n",
      "  Vocabulary size 11541\n",
      "  Most frequent words: [(',', 12045), ('the', 9657), ('of', 6137), ('and', 6122), ('.', 5712), ('to', 4829), ('i', 4312), ('a', 2711), (';', 2662), ('my', 2659), ('in', 2597), ('was', 2249), ('that', 2091), ('her', 1657), ('his', 1646), ('with', 1529), ('he', 1484), ('me', 1473), ('had', 1330), ('not', 1189), ('it', 1180), ('but', 1172), ('for', 1131), ('as', 1097), ('on', 1044), ('you', 1044), ('by', 995), ('from', 968), ('which', 961), ('she', 924)]\n"
     ]
    }
   ],
   "source": [
    "writers = ['EAP', 'HPL', 'MWS']\n",
    "for writer in writers:\n",
    "    print('\\nFor writer', writer)\n",
    "    allTokens[writer] = tokenList(d[d.author==writer].tokens)\n",
    "    print('  Number of tokens', len(allTokens[writer]))\n",
    "    vocabulary[writer] = set(allTokens[writer])\n",
    "    print('  Vocabulary size', len(vocabulary[writer]))\n",
    "    tokenFreq[writer] = frequencies(allTokens[writer])\n",
    "    print('  Most frequent words:', tokenFreq[writer][:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Tokens occuring only once 9307\n",
      "Samples of rare words ['a.d', 'a.d.', 'a.m', 'aaem', 'ab', 'abaft', 'abased', 'abasement', 'abashed', 'abashment']\n"
     ]
    }
   ],
   "source": [
    "singletons = list(filter(lambda x: x[1] == 1, tokenFreq['corpus']))\n",
    "print(\"Number of Tokens occuring only once\", len(singletons))\n",
    "print(\"Samples of rare words\", list(map(first, singletons[:10])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple model on most frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "dv = DictVectorizer()\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 100 most frequent tokens are [',', 'the', 'of', '.', 'and', 'to', 'i', 'a', 'in', 'was', 'that', 'my', ';', 'it', 'he', 'had', 'with', 'his', 'as', 'for', 'not', 'which', 'but', 'at', 'me', 'from', 'by', '``', 'is', 'this', 'on', 'be', 'her', 'were', 'have', 'all', 'you', 'an', 'we', 'or', 'no', 'one', 'so', 'him', 'when', 'they', 'been', 'upon', 'there', 'could', 'she', 'its', 'would', 'more', 'now', 'their', '?', 'what', 'some', 'our', 'are', 'into', 'than', 'will', 'very', 'who', 'if', 'them', 'only', 'then', 'up', 'these', 'before', 'man', 'about', 'any', 'time', 'did', 'yet', 'out', 'said', 'even', 'your', 'might', 'after', 'do', 'old', 'like', 'can', 'first', 'must', 'us', 'most', 'through', 'over', 'never', 'life', 'night', 'made', 'other']\n",
      "  with a coverage of 335389\n"
     ]
    }
   ],
   "source": [
    "# Vectorize\n",
    "def first_n_most_frequent_tokens(n):\n",
    "    return list(map(lambda x: x[0], tokenFreq['corpus'][:n]))\n",
    "\n",
    "n = 100\n",
    "print(\"The\", n, \"most frequent tokens are\", first_n_most_frequent_tokens(n))\n",
    "print(\"  with a coverage of\", reduce(lambda x, y: x + y[1],\n",
    "                                         tokenFreq['corpus'][:n], 0))\n",
    "\n",
    "filter_by_set = lambda coll: compose(list,\n",
    "                                     partial(filter, lambda x: x in coll))\n",
    "\n",
    "def frequencies_in_tokens(tokens, vocabulary):\n",
    "    \"Returns the frequencies of the vocs in the tokens as sparse matrix\"\n",
    "    return dv.fit_transform(map(compose(dict,\n",
    "                                     frequencies,\n",
    "                                     filter_by_set(vocabulary)),\n",
    "                            tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = lambda x, y: sum(x == y) / len(x)\n",
    "\n",
    "def classifier_assessment(model_class, X, Y) -> None:\n",
    "    return {'in-sample-precision': precision(Y, model_class.fit(X,Y).predict(X)),\n",
    "            'out-of-sample-precision': np.mean(cross_val_score(model_class, X, Y, cv=8))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        EAP       0.67      0.68      0.67      7900\n",
      "        HPL       0.59      0.66      0.62      5635\n",
      "        MWS       0.67      0.59      0.63      6044\n",
      "\n",
      "avg / total       0.65      0.65      0.65     19579\n",
      "\n",
      "Confusion Matrix\n",
      " ['EAP', 'HPL', 'MWS'] \n",
      " [[5350 1414 1136]\n",
      " [1307 3701  627]\n",
      " [1325 1135 3584]]\n"
     ]
    }
   ],
   "source": [
    "Y = d.author\n",
    "X = frequencies_in_tokens(d.tokens,\n",
    "                          first_n_most_frequent_tokens(200))\n",
    "model = MultinomialNB().fit(X,Y)\n",
    "Y_pred = model.predict(X)\n",
    "print(classification_report(Y, Y_pred))\n",
    "print('Confusion Matrix\\n', writers, '\\n', confusion_matrix(Y, Y_pred, labels=writers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sensitivity to Vocabulary Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 {'in-sample-precision': 0.45559017314469585, 'out-of-sample-precision': 0.45605126059258444}\n",
      "10 {'in-sample-precision': 0.4691250829970887, 'out-of-sample-precision': 0.4692259731151678}\n",
      "50 {'in-sample-precision': 0.5732672761632361, 'out-of-sample-precision': 0.5711207508283209}\n",
      "100 {'in-sample-precision': 0.5990091424485418, 'out-of-sample-precision': 0.5963534444116483}\n",
      "200 {'in-sample-precision': 0.6453342867357883, 'out-of-sample-precision': 0.6388973395539028}\n",
      "400 {'in-sample-precision': 0.6934981357576996, 'out-of-sample-precision': 0.6860909748901042}\n",
      "600 {'in-sample-precision': 0.7227641861177793, 'out-of-sample-precision': 0.7119357667044304}\n",
      "1000 {'in-sample-precision': 0.7562183972623729, 'out-of-sample-precision': 0.7413032593343052}\n",
      "1500 {'in-sample-precision': 0.7820113386791971, 'out-of-sample-precision': 0.7619378174824729}\n",
      "2000 {'in-sample-precision': 0.8008069870779917, 'out-of-sample-precision': 0.7786915094976463}\n",
      "5000 {'in-sample-precision': 0.8575514581950049, 'out-of-sample-precision': 0.822717144727947}\n",
      "10000 {'in-sample-precision': 0.8873282598702692, 'out-of-sample-precision': 0.8414120732064454}\n"
     ]
    }
   ],
   "source": [
    "for n in [5, 10, 50, 100, 200, 400, 600, 1000, 1500, 2000, 5000, 10000]:\n",
    "    X = frequencies_in_tokens(d.tokens, first_n_most_frequent_tokens(n))\n",
    "    print(n, classifier_assessment(MultinomialNB(), X, Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take no more than 200 words as from then on content bearings words appear, e.g. character names like Raymond.\n",
    "\n",
    "### Sensitivity towards Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True) \n",
      " {'in-sample-precision': 0.6453342867357883, 'out-of-sample-precision': 0.6388973395539028}\n",
      "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "     verbose=0) \n",
      " {'in-sample-precision': 0.6685734715766893, 'out-of-sample-precision': 0.6583093490692019}\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False) \n",
      " {'in-sample-precision': 0.9847285356759794, 'out-of-sample-precision': 0.5879765662081782}\n",
      "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "              learning_rate=0.1, loss='deviance', max_depth=5,\n",
      "              max_features=None, max_leaf_nodes=None,\n",
      "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "              min_samples_leaf=1, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False) \n",
      " {'in-sample-precision': 0.7260840696664794, 'out-of-sample-precision': 0.6623944220724295}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "n = 200\n",
    "X = frequencies_in_tokens(d.tokens,\n",
    "                          first_n_most_frequent_tokens(n))\n",
    "\n",
    "model_classes = [MultinomialNB(),\n",
    "                 LinearSVC(),\n",
    "                 RandomForestClassifier(),\n",
    "                 GradientBoostingClassifier(n_estimators= 100, max_depth = 5)]\n",
    "\n",
    "for model_class in model_classes:\n",
    "    print(model_class, '\\n', classifier_assessment(model_class, X, Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NearestCentroid(metric='euclidean', shrink_threshold=None) \n",
      " {'in-sample-precision': 0.477858930486746, 'out-of-sample-precision': 0.47617730584202467}\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=10, p=2,\n",
      "           weights='uniform') \n",
      " {'in-sample-precision': 0.6090198682261607, 'out-of-sample-precision': 0.5193303149288042}\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=20, p=2,\n",
      "           weights='uniform') \n",
      " {'in-sample-precision': 0.5833290770723735, 'out-of-sample-precision': 0.5329662552610975}\n"
     ]
    }
   ],
   "source": [
    "# Which of the common words have the most predictive power\n",
    "from sklearn.neighbors import NearestCentroid, KNeighborsClassifier\n",
    "for model_class in [NearestCentroid(),\n",
    "                    KNeighborsClassifier(n_neighbors=10),\n",
    "                    KNeighborsClassifier(n_neighbors=20)]:\n",
    "    print(model_class, '\\n', classifier_assessment(model_class, X, Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multinomial Bayes Classifier and its Interpretation\n",
    "## Model\n",
    "In an multinomial model the random variable of feature occurrence counts $X = (X_1, \\ldots, X_n)$ has the *distribution conditional on class* $Y$\n",
    "\n",
    "$$ P_k \\left\\{ X_1 = x_1,\\ldots, X_n =x_n | Y \\right\\} = \\frac{m!}{x_1!\\ldots x_n!} \\prod_{k=1\\dots n} p_{k|y}^{x_k}$$\n",
    "\n",
    "**Interpretation:** Using this model implies that a writer $y$ builds a sentence of length $m$ by picking $m$-times from the set of common words where each common word $k$ appears with probability $p_{k|y}$.\n",
    "\n",
    "## Estimation of Model Parameters\n",
    "Sklearn *estimates* these probabilities (recalling source code MultinomialNB._count) by the relative frequencies in the corpus for a class $y$\n",
    "\n",
    "\\begin{align}\n",
    " \\hat p_{k|y} & = \\frac{c_{k,y}}{c_y} \\\\\n",
    "      c_{k,y} & = \\sum_{\\omega| Y(\\omega) = y} X_k(\\omega) + \\alpha \\\\\n",
    "      c_y     & = \\sum_k c_{k|y}\n",
    "\\end{align}\n",
    "The parameter $\\alpha$ regularizes.\n",
    "\n",
    "**Interpretation:**\n",
    "* The writer picks randomly a word from all the words he has ever written \n",
    "* puts it back and repeats $m$-times\n",
    "\n",
    "![alt text](img/um.jpg)\n",
    "\n",
    "Note: This is a portfolio approach in which the occurrences among single observations are irrelevant, e.g. observing two times a frequency of 2 yielding the same as 0 and 4 each one time.\n",
    "\n",
    "## Model Application\n",
    "The calibrated Bayes model attributes observation $x$ to the probability being of class $y$ \n",
    "$$P\\{ Y=y | X=x \\} = \\frac{P\\{Y=y\\} P\\{X=x|Y=y\\}} {P\\{X=x\\}} $$\n",
    "\n",
    "With evidence the same for all classes a Bayes classifier would attribute the class with the highest probability\n",
    "\n",
    "$$\\text{classifier:} ~~ x \\longrightarrow \\text{argmax}_{y} P\\{Y=y\\} P\\{X=x | Y=y\\}$$\n",
    "\n",
    "The latter returns the same when transforming the probabilities in the following way\n",
    "\n",
    "\\begin{align}\n",
    " & P\\{ Y=y \\} P\\{ X=x | Y=y \\} \\\\\n",
    " & = P\\{ Y=y \\}  \\frac{m!}{x_1!\\ldots x_n!} \\prod_{k=1\\dots n} p_{k|y}^{x_k} \\\\\n",
    " & \\approx P\\{ Y=y \\} \\prod_{k=1\\dots n} p_{k|y}^{x_k} \\\\\n",
    " & \\approx \\log P\\{ Y=y \\} + x_k \\log p_{k|y}\n",
    "\\end{align}\n",
    "\n",
    "## Features with high predictive power\n",
    "The higher conditional probability of a word given an author $\\log p_{k|y} $, the higher is its influence on the classifier\n",
    "Ignoring the relative even priors, the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EAP\n",
      "Highest Log prob\n",
      "[(',', -2.131595505653239), ('thing', -2.2931620779952517), ('where', -2.805213123849642), ('the', -2.957860085869676), ('he', -3.2527969089656104)]\n",
      "Lowest\n",
      "[('most', -7.9557218368274984), ('house', -8.122775921490664), ('they', -8.911233281854935), ('light', -9.60438046241488), ('once', -11.906965555408926)]\n",
      "HPL\n",
      "Highest Log prob\n",
      "[('thing', -2.250415344817034), (',', -2.4926255587205084), ('he', -2.834167721975522), ('where', -2.876363836096308), ('the', -2.9004238456752507)]\n",
      "Lowest\n",
      "[('three', -7.994699762782565), ('come', -8.777459102032196), ('will', -8.910990494656719), ('most', -9.470606282592142), ('once', -10.451435535603869)]\n",
      "MWS\n",
      "Highest Log prob\n",
      "[(',', -2.3123414291337845), ('thing', -2.533287496303963), ('where', -2.986575127557769), ('he', -2.9890219112594956), ('the', -3.0583298040129883)]\n",
      "Lowest\n",
      "[('love', -7.880187966248355), ('death', -7.924639728819189), ('know', -8.412992496733121), ('few', -8.412992496733121), ('day', -11.70882936273745)]\n"
     ]
    }
   ],
   "source": [
    "common_words = first_n_most_frequent_tokens(n)\n",
    "log_prob = dict(zip(model.classes_,\n",
    "             map(lambda xs: list(sorted(zip(common_words, xs),\n",
    "                                 key=lambda x: x[1], reverse=True)),\n",
    "                 model.feature_log_prob_)))\n",
    "\n",
    "for w in writers:\n",
    "    print(w)\n",
    "    print('Highest Log prob')\n",
    "    print(log_prob[w][:5])\n",
    "    print('Lowest')\n",
    "    print(log_prob[w][-5:])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Enhance Features with Statistics\n",
    "Add to the above frequencies of common words the following statistics\n",
    " * sentence length\n",
    " \n",
    "The stastistics alone has predictive power. Find a ways for incorporation into above model approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'in-sample-precision': 0.40349353899586293,\n",
       " 'out-of-sample-precision': 0.4034935578860775}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_lengths = np.array(list(map(len, d.tokens))).reshape(-1, 1)\n",
    "classifier_assessment(MultinomialNB(), sentence_lengths, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#X = frequencies_in_tokens(d.tokens, first_n_most_frequent_tokens(n))\n",
    "type(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Questions\n",
    " * Why does the usual tf-idf approach not work for author attribution?\n",
    " * Explain why the in-sample and ou-of-sample error for random forest are so significant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlook\n",
    "\n",
    "Character Level\n",
    " * n-grams \n",
    "   * eg. 3-grams |No_| |o_o| |_on| |one|\n",
    " * punctuation\n",
    "   * eg. frequency of commas, semicolons, periods, quotation-marks\n",
    "\n",
    "Lexical Level\n",
    " * word occurences\n",
    "   * frequences\n",
    "   * indicators\n",
    " * word n-grams\n",
    "\n",
    "Syntactical Level\n",
    " * part of speech\n",
    "   word classes as lexical items: noun, verb, adjective, adverb, pronoun, preposition, conjunction, interjection, numeral, article, determiner\n",
    " * part of sentence\n",
    "   consituents as lexical items: subject, predicate, direct/indirect object, modifier, abvervial,\n",
    " * sentence \n",
    "\n",
    "Semantic\n",
    " *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the above multinomial model the use of common words in a sentence happens in the following way\n",
    "# A writer y picks a common words x_j with probability p_j\n",
    "#  \n",
    "# https://github.com/scikit-learn/scikit-learn/blob/55bf5d9/sklearn/naive_bayes.py#L630"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
